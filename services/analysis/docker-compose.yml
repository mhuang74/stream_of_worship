version: '3.8'

services:
  analysis:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - TARGETPLATFORM=${TARGETPLATFORM:-linux/amd64}
    ports:
      - "8000:8000"
    environment:
      - SOW_R2_BUCKET=${SOW_R2_BUCKET}
      - SOW_R2_ENDPOINT_URL=${SOW_R2_ENDPOINT_URL}
      - SOW_R2_ACCESS_KEY_ID=${SOW_R2_ACCESS_KEY_ID}
      - SOW_R2_SECRET_ACCESS_KEY=${SOW_R2_SECRET_ACCESS_KEY}
      - SOW_ANALYSIS_API_KEY=${SOW_ANALYSIS_API_KEY}
      - SOW_MAX_CONCURRENT_JOBS=${SOW_MAX_CONCURRENT_JOBS:-2}
      - SOW_DEMUCS_DEVICE=${SOW_DEMUCS_DEVICE:-cpu}
      - NATTEN_LOG_LEVEL=error
      # LLM Configuration for LRC generation
      - SOW_LLM_API_KEY=${SOW_LLM_API_KEY}
      - SOW_LLM_BASE_URL=${SOW_LLM_BASE_URL}
      - SOW_LLM_MODEL=${SOW_LLM_MODEL}
      # Whisper Configuration
      - SOW_WHISPER_DEVICE=${SOW_WHISPER_DEVICE:-cpu}
    volumes:
      - analysis-cache:/cache
    deploy:
      resources:
        reservations:
          devices:
            # GPU block requires nvidia-container-toolkit.
            # For CPU-only fallback, remove the entire deploy.resources block.
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

volumes:
  analysis-cache:
