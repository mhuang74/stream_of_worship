version: '3.8'

services:
  analysis:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - TARGETPLATFORM=${TARGETPLATFORM:-linux/amd64}
    ports:
      - "8000:8000"
    environment:
      - SOW_R2_BUCKET=${SOW_R2_BUCKET}
      - SOW_R2_ENDPOINT_URL=${SOW_R2_ENDPOINT_URL}
      - SOW_R2_ACCESS_KEY_ID=${SOW_R2_ACCESS_KEY_ID}
      - SOW_R2_SECRET_ACCESS_KEY=${SOW_R2_SECRET_ACCESS_KEY}
      - SOW_ANALYSIS_API_KEY=${SOW_ANALYSIS_API_KEY}
      - SOW_MAX_CONCURRENT_JOBS=${SOW_MAX_CONCURRENT_JOBS:-2}
      - SOW_DEMUCS_DEVICE=${SOW_DEMUCS_DEVICE:-cpu}
      - NATTEN_LOG_LEVEL=error
      # LLM Configuration for LRC generation
      - SOW_LLM_API_KEY=${SOW_LLM_API_KEY}
      - SOW_LLM_BASE_URL=${SOW_LLM_BASE_URL}
      - SOW_LLM_MODEL=${SOW_LLM_MODEL}
      # Whisper Configuration
      - SOW_WHISPER_DEVICE=${SOW_WHISPER_DEVICE:-cpu}
    volumes:
      - analysis-cache:/cache
    # GPU support: Uncomment the 'deploy' section below if you have a GPU
    # and NVIDIA Container Toolkit installed. See README.md for details.
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [ gpu ]

volumes:
  analysis-cache:
